INFO:     Started server process [165382]
INFO:     Waiting for application startup.
[QwenASR] Loading Qwen/Qwen3-ASR-1.7B on cuda:0â€¦
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
ERROR:    Traceback (most recent call last):
  File "/home/eth/miniconda3/envs/awabot-qwen-asr/lib/python3.10/site-packages/starlette/routing.py", line 694, in lifespan
    async with self.lifespan_context(app) as maybe_state:
  File "/home/eth/miniconda3/envs/awabot-qwen-asr/lib/python3.10/site-packages/fastapi/routing.py", line 226, iINFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8083 (Press CTRL+C to quit)
[QwenASR] Model loaded.
INFO:     127.0.0.1:47986 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:58772 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:35620 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:35620 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:35620 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56822 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:33668 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:33668 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56766 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:41720 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:41720 - "GET /health HTTP/1.1" 200 OK
    _error_msgs, disk_offload_index = load_shard_file(args)
  File "/home/eth/miniconda3/envs/awabot-qwen-asr/lib/python3.10/site-packages/transformers/modeling_utils.py", line 843, in load_shard_file
    disk_offload_index = _load_state_dict_into_meta_model(
  File "/home/eth/miniconda3/envs/awabot-qwen-asr/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 124, in decorate_context
    return func(*args, **kwargs)
  File "/home/eth/miniconda3/envs/awabot-qwen-asr/lib/python3.10/site-packages/transformers/modeling_utils.py", line 748, in _load_state_dict_into_meta_model
    param = param[...]
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 594.00 MiB. GPU 0 has a total capacity of 7.53 GiB of which 23.19 MiB is free. Process 165116 has 2.26 GiB memory in use. Process 164989 has 4.48 GiB memory in use. Including non-PyTorch memory, this process has 720.00 MiB memory in use. Of the allocated memory 606.63 MiB is allocated by PyTorch, and 11.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

ERROR:    Application startup failed. Exiting.
